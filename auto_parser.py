import pandas as pd
import numpy as np
import pyshark as ps
import os
from glob import glob
from tqdm import tqdm
import numpy as np
import random
import math
from datetime import datetime
from datetime import timedelta
from scipy import stats
import lpl
import time
import json
import sys
from utils import *

LOGGER = lpl.getLogger('parser')
    
def read_pcap(filename, filter):
    cap = ps.FileCapture(filename, display_filter=filter, keep_packets=False, use_json=False)
    train_data = {"ip_len": [], "protocol": [], "flags": [], "src_ip": [], "dst_ip": [], "time_to_live": [],
                  "transport_len":[], "captured_len":[], "highest_layer":[], "number":[], "timestamp":[],
                  "dst_port": [], "src_port": [], "tcp_next_seq": [], "tcp_seq_num": [], "tcp_stream": [],
                  "tcp_ack": [], "time_since_first_frame_tcp": [], "time_since_prev_frame_tcp": [],
                  "http_cookie": [], "http_host": [], "http_user_agent": [], 
                  "http_request_uri_query": [], "ssl_record_version": [], "ssl_handshake_cipherspec": [],
                  "ssl_handshake_session_id_length": [], "http_referer": [], "http_req_method": [], "frame_num": []}
    j = 0
    while True:
        try:
            a = cap.next()
            j+=1
            if j % 100000 == 0:
                LOGGER.debug("%s packets proceeded" % j)
        except: 
            for key, value in train_data.copy().items():
                if len(value) == 0:
                    del train_data[key]
            return train_data
        try:
            n = a.tcp.field_names
        except:
            try:
                n = a.udp.field_names
            except:
                continue
        try:
            train_data["ip_len"].append(int(a.ip.len))
        except: continue
        train_data["timestamp"].append(a.sniff_timestamp)
        train_data["frame_num"].append(j)
        train_data["number"].append(a.number)
        train_data["highest_layer"].append(a.highest_layer)
        try:
            train_data["captured_len"].append(int(a.captured_length))
        except:
            train_data["captured_len"].append(0)
        train_data["protocol"].append(a.ip.proto)
        train_data["flags"].append(a.ip.flags)
        train_data["src_ip"].append(a.ip.src)
        train_data["dst_ip"].append(a.ip.dst)
        train_data["time_to_live"].append(int(a.ip.ttl))
        try:
            train_data["dst_port"].append(int(a.udp.dstport))
            train_data["src_port"].append(int(a.udp.srcport))
            train_data["time_since_first_frame_tcp"].append(-1)
            train_data["time_since_prev_frame_tcp"].append(-1)
        except Exception as ep:
            try:
                train_data["dst_port"].append(int(a.tcp.dstport))
                train_data["src_port"].append(int(a.tcp.srcport))
                try:
                    train_data["time_since_first_frame_tcp"].append(float(a.tcp.time_relative))
                except:
                    train_data["time_since_first_frame_tcp"].append(-1)
                try:
                    train_data["time_since_prev_frame_tcp"].append(float(a.tcp.time_delta))           
                except:
                    train_data["time_since_prev_frame_tcp"].append(-1)
            except Exception as e: 
                pass
        try:
            train_data["http_host"].append(a.http.host)
        except Exception as e:
            train_data["http_host"].append(0)
        try:
            train_data["http_cookie"].append(a.http.cookie)
        except: train_data["http_cookie"].append(0)
        try:
            train_data["http_req_method"].append(a.http.request_method)
        except: train_data["http_req_method"].append(0)
        try:
            train_data["http_user_agent"].append(a.http.user_agent)
        except: 
            train_data["http_user_agent"].append(0)
        try:
            train_data["http_request_uri_query"].append(a.http.request_uri_query)
        except: 
            train_data["http_request_uri_query"].append(0)
        try:
            train_data["ssl_record_version"].append(a.ssl.record_version)
        except: 
            train_data["ssl_record_version"].append(0)
        try:
            train_data["ssl_handshake_cipherspec"].append(a.ssl.handshake_cipherspec)
        except: 
            train_data["ssl_handshake_cipherspec"].append(0)
        try:
            train_data["ssl_handshake_session_id_length"].append(a.ssl.handshake_session_id_length)
        except: 
            train_data["ssl_handshake_session_id_length"].append(0)
        try:
            train_data["http_referer"].append(a.http.referer)
        except: 
            train_data["http_referer"].append(0)
    
def generate_feature_df(data):
    time_since_first = []
    is_first = []
    captured_len_mean= []
    ssl_len_mean = []
    time_between_frames_mean = []
    for index, row in tqdm(data.iterrows()):
        search = data[(data["src_ip"]==row["src_ip"]) & (data["dst_ip"]==row["dst_ip"]) & (data["src_port"]==row["src_port"]) & (data["dst_port"]==row["dst_port"]) & (data["timestamp"]<=row["timestamp"])]
        LOGGER.debug(search.shape)
        time_between_frames_mean.append(search[search["time_since_prev_frame_tcp"]>0]["time_since_prev_frame_tcp"].mean())
        captured_len_mean.append(search["captured_len"].mean())
        ssl_len_mean.append(search["ssl_handshake_session_id_length"].mean())
        if len(search) > 1:
            is_first.append(1)
        else:
            is_first.append(0)
        prev = list(search[search["time_since_prev_frame_tcp"]>0]["timestamp"])
        try:
            prev.pop(0)
            prev.append(0)
            mean_t = search[search["time_since_prev_frame_tcp"]>0]["timestamp"] - prev
            time_since_first.append(prev[-2]-prev[0])
        except:
            time_since_first.append(0)
            
    data["time_since_first"] = time_since_first
    data["is_first"] = is_first
    data["captured_len_mean"] = captured_len_mean
    data["time_between_frames_mean"] = time_between_frames_mean  
    data["ssl_len_mean"] = ssl_len_mean
    data = data.fillna(0)
    return data
    
def prepare_ids_log(path):
    import os
    from glob import glob
    import re
    import pandas as pd
    from datetime import datetime
    import dateutil
    
    data_dict = {"timestamp": [], "warning": [], "src_ip": [], "dst_ip": [], "transmission_protocol": [], "class": []}
    for line in open(path, 'r'):
        splitted = line.split(" ")
        data_dict["dst_ip"].append(splitted[-1][0:-1])
        data_dict["src_ip"].append(splitted[-3][0:])
        data_dict["timestamp"].append(dateutil.parser.parse(splitted[0][:]))
        data_dict["transmission_protocol"].append(splitted[-4][1:-1])
        for i, element in enumerate(splitted):
            if "[1:" in element:
                try:
                    data_dict["warning"].append(int(element[3:-4]))
                except: 
                    data_dict["warning"].append(int(element[3:-6]))
            if "Classification:" in element:
                l = 1
                data = ""
                while not ("]" in splitted[i+l-1]):  
                    data += splitted[i+l]
                    l+=1
                data_dict["class"].append(data[:-1])
    data = pd.DataFrame.from_dict(data_dict)
    return data

def insert_commas(file_name):
    dummy_file = file_name + '.bak'
    f = open(file_name, 'r')
    thresh = len(f.readlines())
    f.close()
    with open(file_name, 'r') as read_obj, open(dummy_file, 'w') as write_obj:
        i = 0
        for line in read_obj:
            if i in range(1, thresh-3):
                write_obj.write(line+",")
            else:
                write_obj.write(line)
            i+=1
    os.remove(file_name)
    os.rename(dummy_file, file_name)
    
def prepend_line(file_name, line):
    dummy_file = file_name + '.bak'
    with open(file_name, 'r') as read_obj, open(dummy_file, 'w') as write_obj:
        write_obj.write(line + '\n')
        for line in read_obj:
            write_obj.write(line)
        write_obj.write("\n]}")    
    os.remove(file_name)
    os.rename(dummy_file, file_name)
    
def prepare_eve(path):
    prepend_line(path, "{\"info\": [")
    insert_commas(path)
    return read_config(path)
    
def prepare_ids_log_alt(config):
    import os
    from glob import glob
    import re
    import pandas as pd
    from datetime import datetime
    import dateutil
    
    data_dict = {"timestamp": [], "warning": [], "src_ip": [], "dst_ip": [], "src_port": [], "dst_port": [], "class": [], "frame_num": []}
    alerts = []
    for element in config["info"]:
        if "alert" in element.keys():
            if ("was" in element["alert"]["category"]) or ("Misc Attack" in element["alert"]["category"]) or ("Malware Command and Control Activity Detected" in element["alert"]["category"]) or ("Misc activity" in element["alert"]["category"]) :
                try:
                    data_dict["frame_num"].append(element["pcap_cnt"])
                except:
                    continue
                data_dict["class"].append(element["alert"]["category"])
                data_dict["src_ip"].append(element["src_ip"])
                data_dict["dst_ip"].append(element["dest_ip"])                                                                                              
                data_dict["timestamp"].append(element["timestamp"])
                data_dict["warning"].append(element["event_type"])
                data_dict["src_port"].append(element["src_port"])
                data_dict["dst_port"].append(element["dest_port"])                                                                                          
                                                                                                              
    data = pd.DataFrame.from_dict(data_dict)
    return data
    
def generate_dataframe(data, out_path="train_output_2018-12-09.csv"):
    if not os.path.exists("temp.csv"):
        train = pd.DataFrame.from_dict(data)
        train.to_csv("temp.csv")
    else:
        train = pd.read_csv("temp.csv")
    config = prepare_eve("eve.json")
    fast = prepare_ids_log_alt(config)
    LOGGER.debug(fast.shape)
    train["datetime"] = train["timestamp"].apply(lambda x: datetime.utcfromtimestamp(float(x)))
    train["datetime"] = train["datetime"].apply(lambda x: convert_microsec(x))
    unnormal = train[train["frame_num"].isin(fast["frame_num"].unique())]
    unnormal["traffic_class"] = "ANetworkTrojanwasdetected"
    LOGGER.debug(unnormal.shape)
    normal = train[~train["frame_num"].isin(fast["frame_num"].unique())]
    normal = normal.groupby(['src_ip', "src_port", 'dst_ip', "dst_port"]).apply(lambda x: x.sample(1)).reset_index(drop=True)
    LOGGER.debug("generating features ...")
    train = pd.concat([generate_feature_df(normal)])
    train["src_port_type"] = train["src_port"].apply(lambda x: 1 if x in range(49152,65535) else 0)
    train["dst_port_type"] = train["dst_port"].apply(lambda x: 1 if x in range(49152,65535) else 0)
    del train["timestamp"]
    train["flags"] = train["flags"].apply(lambda x: encode_flags(x))
    train1 = pd.get_dummies(train, columns=["highest_layer"])
    train1["src_port_less_1023"] = train1["src_port"].apply(lambda x: official_ports_(x))
    train1["dst_port_less_1023"] = train1["dst_port"].apply(lambda x: official_ports_(x))
    train1["src_port_http"] = train1["src_port"].apply(lambda x: official_ports__(x))
    train1["dst_port_http"] = train1["dst_port"].apply(lambda x: official_ports__(x))
    train1["src_port_https"] = train1["src_port"].apply(lambda x: official_ports___(x))
    train1["dst_port_https"] = train1["dst_port"].apply(lambda x: official_ports___(x))
    train1["is_internal_ip"] = train1["src_ip"].apply(lambda x: internal_ip(x))
    train1["is_internal_ip_dst"] = train1["dst_ip"].apply(lambda x: internal_ip(x))
    train1["is_internal_traffic"] = train1["is_internal_ip"]+train1["is_internal_ip_dst"]
    train1["is_internal_traffic"] = train1["is_internal_traffic"].apply(lambda x: internal_traffic(x))
    train1["protocol"] = train1["protocol"].apply(lambda x: encode_(x))
    train1 = normilize_features(train1, ["captured_len", "captured_len_mean", "time_since_first",
                                        "time_to_live", "ssl_len_mean", "ssl_handshake_session_id_length"])
    LOGGER.debug("saving ...")
    train1.to_csv(out_path)
    
def normilize_features(data, features):
    for feature in features:
        try:
            data[feature] = (data[feature]-data[feature].min())/(data[feature].max()-data[feature].min())
        except: continue
    return data
    
if __name__ == "__main__":
    LOGGER.debug("reading pcap")
    if not os.path.exists("temp.csv"):
        parsed_pcap = read_pcap(str(sys.argv[1]), "")
        LOGGER.debug("generating dataframe ...")
        generate_dataframe(parsed_pcap)
    else:
        LOGGER.debug("generating dataframe ...")
        generate_dataframe("")